{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ef1e06",
   "metadata": {},
   "source": [
    "### SingLEM: Feature Extraction Demo\n",
    "\n",
    "This notebook provides a hands-on guide to using the pretrained SingLEM model for feature extraction. We will cover two main examples:\n",
    "1. A simple dummy data example: How to load the model and pass a random tensor through it to understand the input and output shapes.\n",
    "2. A real EEG data example: A realistic workflow showing how to load a raw EEG file, apply the necessary preprocessing, and extract features for all channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5775b78",
   "metadata": {},
   "source": [
    "### 1. Setup and Model Loading\n",
    "First, we'll set up the necessary paths, import the model architecture, and load the pretrained weights. The model will be automatically moved to a GPU if one is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "981f7558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SingLEM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1668582/3620931688.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder_state = torch.load(weights_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on 'cuda'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import mne\n",
    "import torch\n",
    "\n",
    "# --- Setup Paths ---\n",
    "# This assumes the notebook is in the 'examples' directory\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.dirname(current_dir)\n",
    "# Add the 'singlem' package directory to the Python path\n",
    "singlem_path = os.path.join(project_root, 'SingLEM')\n",
    "sys.path.append(singlem_path)\n",
    "\n",
    "# --- Import Model and Define Weights Path ---\n",
    "from model import EEGEncoder, Config\n",
    "weights_path = os.path.join(project_root, 'weights', 'singlem_pretrained.pt')\n",
    "\n",
    "# --- Initialize Model and Load Weights ---\n",
    "print(\"Loading SingLEM model...\")\n",
    "config = Config()\n",
    "config.mask_prob = 0.0  # Set mask probability to 0 for feature extraction\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "feature_extractor = EEGEncoder(config)\n",
    "\n",
    "encoder_state = torch.load(weights_path, map_location=device)\n",
    "feature_extractor.load_state_dict(encoder_state)\n",
    "feature_extractor = feature_extractor.to(device)\n",
    "feature_extractor.eval()    # Set the model to evaluation mode\n",
    "print(f\"Model loaded successfully on '{device}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e63f7a",
   "metadata": {},
   "source": [
    "### 2. Dummy Data Example\n",
    "Let's start by creating a random tensor to represent a batch of single-channel EEG sequences and passing it through the model. This is the quickest way to verify that the model is working and to understand its expected input and output shapes.\n",
    "\n",
    "The expected input shape for SingLEM is (batch_size, num_tokens, samples_per_token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302dec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 10, 128])\n",
      "Output feature shape: torch.Size([1, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy tensor: 1 sequence, 10 tokens, 128 samples per token\n",
    "# Shape: (batch_size, num_tokens, samples_per_token)\n",
    "dummy_eeg_sequence = torch.randn(1, 10, 128, device=device)\n",
    "\n",
    "# Extract features:\n",
    "with torch.no_grad():\n",
    "    features, _, _ = feature_extractor(dummy_eeg_sequence)\n",
    "\n",
    "print(f\"Input shape: {dummy_eeg_sequence.shape}\")\n",
    "print(f\"Output feature shape: {features.shape}\")\n",
    "\n",
    "# The output shape is (batch_size, num_tokens, feature_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3e168a",
   "metadata": {},
   "source": [
    "### 3. Real EEG Data Example\n",
    "Now, let's walk through a more realistic example using a sample EEG file. We'll use the MNE-Python library, a standard tool for EEG analysis, to load and preprocess the data.\n",
    "\n",
    "Our goal is to take a multi-channel EEG recording and extract SingLEM features for every channel independently.\n",
    "\n",
    "Note: You will need to have an EEG file (e.g., EEG.gdf) in the same directory as this notebook for this example to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554f3362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original raw data info:\n",
      "<Info | 8 non-empty values\n",
      " bads: []\n",
      " ch_names: Fz, FCz, Cz, CPz, Pz, C1, C3, C5, C2, C4, C6, EOG1, EOG2, EOG3, ...\n",
      " chs: 32 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 256.0 Hz\n",
      " meas_date: unspecified\n",
      " nchan: 32\n",
      " projs: []\n",
      " sfreq: 512.0 Hz\n",
      " subject_info: <subject_info | his_id: X, last_name: >\n",
      ">\n",
      "Preprocessed data info: 27 channels, 128.0 Hz\n",
      "Using data from preloaded Raw for 249 events and 128 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "Data tokenized into shape: (249, 27, 128)\n",
      "Data reshaped for model input: (6723, 1, 128)\n",
      "\n",
      "Extracted features shape: torch.Size([6723, 1, 16])\n",
      "Features reshaped to (channels, tokens, feature_dim): torch.Size([27, 249, 16])\n"
     ]
    }
   ],
   "source": [
    "# --- 3.1. Load and Preprocess EEG Data ---\n",
    "# \"Before feature extraction, the raw EEG data must be preprocessed to match the format SingLEM expects. This involves three key steps:\",\n",
    "# \"1. **Filtering:** A 0.5-50 Hz band-pass filter and a 50 Hz notch filter.\",\n",
    "# \"2. **Resampling:** Downsampling to 128 Hz.\",\n",
    "# \"3. **Amplitude Scaling:** Scaling the data (in Volts) by `1e4` to be in a range of roughly [-1, 1].\"\n",
    "\n",
    "# This example uses a GDF file, but MNE supports many formats (EDF, BDF, BrainVision, etc.)\n",
    "try:\n",
    "    raw = mne.io.read_raw_gdf('EEG.gdf', preload=True, verbose=False)\n",
    "except FileLinks:\n",
    "    print(\"Sample file 'EEG.gdf' not found. Skipping real data example.\")\n",
    "    raw = None\n",
    "if raw:\n",
    "    print(\"Original raw data info:\")\n",
    "    print(raw.info)\n",
    "\n",
    "    # Apply minimal preprocessing\n",
    "    # NOte: These steps should match the preprocessing of SingLEM's pretraining process and your actual data\n",
    "    raw.drop_channels(ch_names=['EOG1', 'EOG2', 'EOG3', 'EMGg', 'EMGd'])\n",
    "    # Step 1: Filtering\n",
    "    raw.notch_filter(50, verbose=False)\n",
    "    raw.filter(0.5, 50, verbose=False)\n",
    "\n",
    "    # Step 2: Resampling\n",
    "    raw.resample(128, verbose=False)\n",
    "\n",
    "    # Step 3: Amplitude Scaling\n",
    "    # The example data is in Volts (V). We scale it by 1e4\n",
    "    scaling_factor = 1e4\n",
    "    raw.apply_function(lambda x: x * scaling_factor, verbose=False)\n",
    "\n",
    "    print(f\"Preprocessed data info: {raw.info['nchan']} channels, {raw.info['sfreq']} Hz\")\n",
    "\n",
    "    # --- 3.2. Tokenize the Data ---\n",
    "    # Next, we segment the continuous data into 1-second tokens. Since SingLEM processes each channel independently,\n",
    "    # we then reshape the data so that all tokens from all channels can be processed in one large batch.\n",
    "    \n",
    "    # The simplest way to tokenize the data is using MNE's make_fixed_length_epochs function.\n",
    "    # We create 1-second epochs (tokens) with a 25% overlap (stride of 0.75s)\n",
    "    epochs = mne.make_fixed_length_epochs(raw, duration=1.0, overlap=0.25, verbose=False)\n",
    "\n",
    "    # Get data as a Numpy array: (num_tokens, num_channels, samples_per_token)\n",
    "    data = epochs.get_data()\n",
    "    print(f\"\\nData tokenized into shape: {data.shape}\")\n",
    "\n",
    "    # --- 3.3. Reshape for SingLEM ---\n",
    "    # SingLEM processes each channel independently. We need to reshape the data\n",
    "    # so that the channel and token/trial dimensions are combined into the batch dimension.\n",
    "    \n",
    "    # First, swap axes to (num_channels, num_tokens, samples_per_token)\n",
    "    data_transposed = data.transpose(1, 0, 2)\n",
    "    \n",
    "    # We will process all tokens from all channels in one large batch.\n",
    "    # New shape: (num_channels * num_tokens, 1, samples_per_token)\n",
    "    # The '1' represents a sequence length of a single token for this simple case.\n",
    "    # For longer sequences, you would group tokens first.\n",
    "    batched_input = data_transposed.reshape(-1, 1, 128)\n",
    "    \n",
    "    print(f\"Data reshaped for model input: {batched_input.shape}\")\n",
    "\n",
    "    # --- 3.4. Extract Features ---\n",
    "    # Convert to a PyTorch tensor and pass through the model\n",
    "    input_tensor = torch.tensor(batched_input, dtype=torch.float32, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features, _, _ = feature_extractor(input_tensor)\n",
    "        \n",
    "    print(f\"\\nExtracted features shape: {features.shape}\")\n",
    "    \n",
    "    # --- 3.5. Reshape Features Back ---\n",
    "    # The output can be reshaped back to separate the channel and token dimensions\n",
    "    # Shape: (num_channels, num_tokens, feature_dimension)\n",
    "    num_channels = data.shape[1]\n",
    "    features_reshaped = features.reshape(num_channels, -1, features.shape[-1])\n",
    "    print(f\"Features reshaped to (channels, tokens, feature_dim): {features_reshaped.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
